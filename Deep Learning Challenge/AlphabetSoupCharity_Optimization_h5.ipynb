{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLZFOBECfsy2"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FjqaeHvfsy6"
      },
      "outputs": [],
      "source": [
        "# import just the first dependency we need to start the timer\n",
        "import time\n",
        "\n",
        "# Start the timer\n",
        "start_the_timer = time.time()\n",
        "\n",
        "# Import our dependencies\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import warnings\n",
        "\n",
        "#  Import and read the charity_data.csv.\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju1_ZrZIfszB"
      },
      "source": [
        "## Compile, Train and Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWeWysCefszC"
      },
      "outputs": [],
      "source": [
        "# set our variables\n",
        "iterations = 0\n",
        "first_hidden_layer = 3\n",
        "second_hidden_layer = 2\n",
        "third_hidden_layer = 0\n",
        "activation_1 = 'tanh'\n",
        "activation_2 = 'relu'\n",
        "activation_3 = 'sigmoid'\n",
        "epochs_count = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4L1YsvhfszC"
      },
      "outputs": [],
      "source": [
        "# Drop the non-beneficial ID columns, 'EIN', 'NAME', 'STATUS', 'SPECIAL_CONSIDERATIONS'\n",
        "optimized_df = df[['NAME',\n",
        "                   'APPLICATION_TYPE',\n",
        "                   'AFFILIATION',\n",
        "                   'CLASSIFICATION',\n",
        "                   'USE_CASE',\n",
        "                   'ORGANIZATION',\n",
        "                   'INCOME_AMT',\n",
        "                   'ASK_AMT',\n",
        "                   'IS_SUCCESSFUL']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhHY7LExfszC",
        "outputId": "2b201ce7-d6f2-4fb7-c44e-4840bde710b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization Run Time: 0h 0m 31s\n"
          ]
        }
      ],
      "source": [
        "# Start measuring time\n",
        "start_time = time.time()\n",
        "\n",
        "# Disable the SettingWithCopyWarning\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "\n",
        "# Define column names and cutoff values in a dictionary\n",
        "column_info = {\"NAME\": {\"cutoff\": 2},\n",
        "               \"APPLICATION_TYPE\": {\"cutoff\": 528},\n",
        "               \"CLASSIFICATION\": {\"cutoff\": 1883},\n",
        "               \"INCOME_AMT\": {\"cutoff\": 24388},\n",
        "               \"ASK_AMT\": {\"cutoff\": 25398}}\n",
        "\n",
        "replacement_value = \"Other\"\n",
        "\n",
        "def replace_low_counts(optimized_df, column_info, replacement_value):\n",
        "    for col, info in column_info.items():\n",
        "        value_counts = optimized_df[col].value_counts()\n",
        "        values_to_replace = list(value_counts[value_counts < info[\"cutoff\"]].index)\n",
        "        optimized_df[col] = optimized_df[col].replace(values_to_replace, replacement_value)\n",
        "\n",
        "# Call your function\n",
        "replace_low_counts(optimized_df, column_info, replacement_value)\n",
        "\n",
        "# Stop measuring time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Convert elapsed time to hours, minutes, and seconds\n",
        "hours, remainder = divmod(elapsed_time, 3600)\n",
        "minutes, seconds = divmod(remainder, 60)\n",
        "\n",
        "# Record the Training time\n",
        "print(f\"Optimization Run Time: {int(hours)}h {int(minutes)}m {int(seconds)}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKN9xZtgfszD"
      },
      "outputs": [],
      "source": [
        "optimized_df = pd.get_dummies(optimized_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d44vNX2HfszD"
      },
      "outputs": [],
      "source": [
        "# Split our preprocessed data into our features and target arrays\n",
        "X = optimized_df.drop('IS_SUCCESSFUL', axis = 1)\n",
        "y = optimized_df['IS_SUCCESSFUL']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the preprocessed data into a training and testing dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "metadata": {
        "id": "qP2KXelfhScj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a StandardScaler instances\n",
        "scaler = StandardScaler()"
      ],
      "metadata": {
        "id": "1Or7lNyGhSs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the StandardScaler\n",
        "X_scaler = scaler.fit(X_train)\n"
      ],
      "metadata": {
        "id": "XIVE9TWwhX-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the data\n",
        "X_train_scaled = X_scaler.transform(X_train)\n",
        "X_test_scaled = X_scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "2t3L6pFchYIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
        "input_features = len(X_train_scaled[0])\n",
        "nn = tf.keras.models.Sequential()\n",
        "\n",
        "# First hidden layer\n",
        "nn.add(tf.keras.layers.Dense(units=first_hidden_layer, input_dim=input_features, activation=activation_1))\n",
        "\n",
        "# Second hidden layer\n",
        "nn.add(tf.keras.layers.Dense(units=second_hidden_layer, activation=activation_2))\n",
        "\n",
        "# Output layer\n",
        "nn.add(tf.keras.layers.Dense(units=1, activation=activation_3))"
      ],
      "metadata": {
        "id": "R-OVTI9fhnfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the structure of the model\n",
        "nn.summary()"
      ],
      "metadata": {
        "id": "e28vcVIChnh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "FXgHPZmkhYUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable the SettingWithCopyWarning\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "# Define the filename\n",
        "# iterations = 3\n",
        "filename = f\"AlphabetSoupCharity_Optimization_{iterations}.h5\"\n",
        "\n",
        "# Start measuring time\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the neural network model\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    nn.fit(X_train_scaled, y_train, epochs=epochs_count, verbose=0)\n",
        "\n",
        "# Stop measuring time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Convert elapsed time to hours, minutes, and seconds\n",
        "hours, remainder = divmod(elapsed_time, 3600)\n",
        "minutes, seconds = divmod(remainder, 60)\n",
        "\n",
        "# Record the Training time\n",
        "print(f\"Training Run Time: {int(hours)}h {int(minutes)}m {int(seconds)}s\")"
      ],
      "metadata": {
        "id": "mL9UgLuMhzIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model using the test data\n",
        "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=0)\n",
        "print(f\"Model Loss: {model_loss}/nModel Accuracy: {model_accuracy}\")"
      ],
      "metadata": {
        "id": "6qvpW4dhhzLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable the SettingWithCopyWarning\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "# Define the filename\n",
        "filename = f\"AlphabetSoupCharity_Optimization_{iterations}.h5\"\n",
        "\n",
        "# Save the neural network model with the constructed filename\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    nn.save(filename)\n",
        "\n",
        "# Download the file\n",
        "files.download(filename)\n",
        "\n",
        "# Direct the user where to find the file\n",
        "print(f\"{filename} has been exported to your Downloads folder.\")\n",
        "\n",
        "# Stop timing us\n",
        "stop_the_timer = time.time()\n",
        "\n",
        "# Calculate entire run time\n",
        "entire_run_time = stop_the_timer - start_the_timer\n",
        "\n",
        "# Convert elapsed time to hours, minutes, and seconds\n",
        "hours, remainder = divmod(entire_run_time, 3600)\n",
        "minutes, seconds = divmod(remainder, 60)\n",
        "\n",
        "# Record the Training time\n",
        "print(f\"Total Run Time: {int(hours)}h {int(minutes)}m {int(seconds)}s\")"
      ],
      "metadata": {
        "id": "lJJVk4hwh4hq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}